{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":23,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# All imports are defined here:-"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"%matplotlib inline\nimport torchvision\nimport torchvision.datasets as dset\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader,Dataset\nimport matplotlib.pyplot as plt\nimport torchvision.utils\nimport numpy as np\nimport random\nfrom PIL import Image\nimport torch\nfrom torch.autograd import Variable\nimport PIL.ImageOps    \nimport torch.nn as nn\nfrom torch import optim\nimport torch.nn.functional as F","execution_count":24,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Defining Training Directories and CSV's:-"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dir=\"/kaggle/input/signature-verification-dataset/sign_data/train\"\ntrain_csv=\"/kaggle/input/signature-verification-dataset/sign_data/train_data.csv\"\ntest_csv=\"/kaggle/input/signature-verification-dataset/sign_data/test_data.csv\"\ntest_dir=\"/kaggle/input/signature-verification-dataset/sign_data/test\"","execution_count":25,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train=pd.read_csv(train_csv)\ndf_train.sample(10)","execution_count":26,"outputs":[{"output_type":"execute_result","execution_count":26,"data":{"text/plain":"       068/09_068.png  068_forg/03_0113068.PNG  1\n2432   004/004_23.PNG  004_forg/0124004_04.png  1\n11376  003/003_01.PNG           003/003_18.PNG  0\n400    033/12_033.png  033_forg/03_0205033.PNG  1\n18489  015/015_09.PNG           015/015_11.PNG  0\n16971  017/10_017.png  017_forg/02_0124017.PNG  1\n1273   062/05_062.png           062/08_062.png  0\n3424   026/05_026.png  026_forg/03_0125026.PNG  1\n6448   002/002_24.PNG           002/002_09.PNG  0\n10710  003/003_11.PNG  003_forg/0206003_03.png  1\n18049  028/12_028.png           028/11_028.png  0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>068/09_068.png</th>\n      <th>068_forg/03_0113068.PNG</th>\n      <th>1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2432</th>\n      <td>004/004_23.PNG</td>\n      <td>004_forg/0124004_04.png</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>11376</th>\n      <td>003/003_01.PNG</td>\n      <td>003/003_18.PNG</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>400</th>\n      <td>033/12_033.png</td>\n      <td>033_forg/03_0205033.PNG</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>18489</th>\n      <td>015/015_09.PNG</td>\n      <td>015/015_11.PNG</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>16971</th>\n      <td>017/10_017.png</td>\n      <td>017_forg/02_0124017.PNG</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1273</th>\n      <td>062/05_062.png</td>\n      <td>062/08_062.png</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3424</th>\n      <td>026/05_026.png</td>\n      <td>026_forg/03_0125026.PNG</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>6448</th>\n      <td>002/002_24.PNG</td>\n      <td>002/002_09.PNG</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>10710</th>\n      <td>003/003_11.PNG</td>\n      <td>003_forg/0206003_03.png</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>18049</th>\n      <td>028/12_028.png</td>\n      <td>028/11_028.png</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"# Here we are seeing that 1 denotes for forged pair and 0 denotes for geniune pair of signatures.."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test=pd.read_csv(test_csv)\ndf_test.sample(10)","execution_count":27,"outputs":[{"output_type":"execute_result","execution_count":27,"data":{"text/plain":"      068/09_068.png  068_forg/03_0113068.PNG  1\n3673  052/05_052.png  052_forg/03_0106052.PNG  1\n2696  069/07_069.png  069_forg/01_0111069.PNG  1\n1410  051/05_051.png           051/02_051.png  0\n1172  067/05_067.png           067/08_067.png  0\n5024  064/05_064.png           064/07_064.png  0\n1310  051/12_051.png  051_forg/03_0120051.PNG  1\n1500  051/09_051.png           051/04_051.png  0\n4183  054/11_054.png           054/03_054.png  0\n1001  065/09_065.png           065/01_065.png  0\n4904  059/04_059.png           059/06_059.png  0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>068/09_068.png</th>\n      <th>068_forg/03_0113068.PNG</th>\n      <th>1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>3673</th>\n      <td>052/05_052.png</td>\n      <td>052_forg/03_0106052.PNG</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2696</th>\n      <td>069/07_069.png</td>\n      <td>069_forg/01_0111069.PNG</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1410</th>\n      <td>051/05_051.png</td>\n      <td>051/02_051.png</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1172</th>\n      <td>067/05_067.png</td>\n      <td>067/08_067.png</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5024</th>\n      <td>064/05_064.png</td>\n      <td>064/07_064.png</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1310</th>\n      <td>051/12_051.png</td>\n      <td>051_forg/03_0120051.PNG</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1500</th>\n      <td>051/09_051.png</td>\n      <td>051/04_051.png</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4183</th>\n      <td>054/11_054.png</td>\n      <td>054/03_054.png</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1001</th>\n      <td>065/09_065.png</td>\n      <td>065/01_065.png</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4904</th>\n      <td>059/04_059.png</td>\n      <td>059/06_059.png</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.shape","execution_count":28,"outputs":[{"output_type":"execute_result","execution_count":28,"data":{"text/plain":"(23205, 3)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.shape","execution_count":29,"outputs":[{"output_type":"execute_result","execution_count":29,"data":{"text/plain":"(5747, 3)"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"# Making Custom Pytorch Siamese Dataset:-"},{"metadata":{},"cell_type":"markdown","source":"the ____len____ function which returns the size of the dataset, and\n\nthe ____getitem____ function which returns a sample from the dataset given an index."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train[4:5]","execution_count":30,"outputs":[{"output_type":"execute_result","execution_count":30,"data":{"text/plain":"   068/09_068.png  068_forg/03_0113068.PNG  1\n4  068/09_068.png  068_forg/04_0113068.PNG  1","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>068/09_068.png</th>\n      <th>068_forg/03_0113068.PNG</th>\n      <th>1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>4</th>\n      <td>068/09_068.png</td>\n      <td>068_forg/04_0113068.PNG</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"image1_path=os.path.join(train_dir,df_train.iat[4,0])\nimage1_path","execution_count":31,"outputs":[{"output_type":"execute_result","execution_count":31,"data":{"text/plain":"'/kaggle/input/signature-verification-dataset/sign_data/train/068/09_068.png'"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Dataset(Dataset):\n    #default constuctor for assigning values\n    def __init__(self,train_dir=None,train_csv=None,transform=None):\n        self.train_dir=train_dir\n        self.train_data=pd.read_csv(train_csv)\n        self.train_data.columns=['image1','image2','class']\n        self.transform=transform\n        \n    def __getitem__(self,idx): ## __getitem__ returns a sample data given index, idx=index\n        \n        img1_path=os.path.join(self.train_dir,self.train_data.iat[idx,0])\n        img2_path=os.path.join(self.train_dir,self.train_data.iat[idx,1])\n        \n        img1=Image.open(img1_path)\n        img2=Image.open(img2_path)\n        \n        img1=img1.convert('L') #L mode image, that means it is a single channel image - normally interpreted as greyscale.\n        img2=img2.convert('L')\n        \n        img1=self.transform(img1)\n        img2=self.transform(img2)\n        \n        return img1, img2, torch.from_numpy(np.array([int(self.train_data.iat[idx,2])],dtype=np.float32))\n    \n    \n    def __len__(self): ## __len__ returns the size of the dataset..\n        return len(self.train_data)","execution_count":32,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Returns Image1, Image2 and the class label(whether 0 or 1)."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load the the dataset from raw image folders\ndataset = Dataset(train_dir,train_csv,transform=transforms.Compose([transforms.Resize((100,100)),transforms.ToTensor()]))","execution_count":33,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset","execution_count":34,"outputs":[{"output_type":"execute_result","execution_count":34,"data":{"text/plain":"<__main__.Dataset at 0x7f8009d1a410>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# def imshow(img,text=None,should_save=False):\n#     npimg = img.numpy()\n#     plt.axis(\"off\")\n#     if text:\n#         plt.text(75, 8, text, style='italic',fontweight='bold',\n#             bbox={'facecolor':'white', 'alpha':0.8, 'pad':10})\n#     plt.imshow(np.transpose(npimg, (1, 2, 0)))\n#     plt.show()    \n\n# def show_plot(iteration,loss):\n#     plt.plot(iteration,loss)\n#     plt.show()","execution_count":35,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Viewing the sample of images and to check whether its loading properly\n# vis_dataloader = DataLoader(dataset,\n#                         shuffle=True,\n#                         batch_size=8)\n# dataiter = iter(vis_dataloader)\n\n\n# example_batch = next(dataiter)\n# concatenated = torch.cat((example_batch[0],example_batch[1]),0)\n# imshow(torchvision.utils.make_grid(concatenated))\n# print(example_batch[2].numpy())","execution_count":36,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Siamese Network:-"},{"metadata":{"trusted":true},"cell_type":"code","source":"class SiameseNetwork(nn.Module):\n    \n    def __init__(self):\n        super(SiameseNetwork, self).__init__()\n        \n        \n        self.conv1=nn.Conv2d(1,50,kernel_size=5)\n        self.pool1 = nn.MaxPool2d(kernel_size = 2, stride = 2, padding = 0)\n         # L1 ImgIn shape=(?, 28, 28, 1)      # (n-f+2*p/s)+1\n        #    Conv     -> (?, 24, 24, 50)\n        #    Pool     -> (?, 12, 12, 50)\n        \n        \n        self.conv2 = nn.Conv2d(50,60, kernel_size = 5)\n        self.pool2 = nn.MaxPool2d(kernel_size = 2, stride = 2, padding = 0)\n        # L2 ImgIn shape=(?, 12, 12, 50)\n        #    Conv      ->(?, 8, 8, 60)\n        #    Pool      ->(?, 4, 4, 60)\n        \n        \n        self.conv3 = nn.Conv2d(60, 80,  kernel_size = 3)\n        # L3 ImgIn shape=(?, 4, 4, 60)\n        #    Conv      ->(?, 2, 2, 80)\n       \n        \n        \n        self.batch_norm1 = nn.BatchNorm2d(50)\n        self.batch_norm2 = nn.BatchNorm2d(60)\n        \n#         self.dropout1 = nn.Dropout2d()\n        \n        # L4 FC 2*2*80 inputs -> 250 outputs\n        self.fc1 = nn.Linear(80*2*2, 250) \n        self.fc2 = nn.Linear(250, 2)\n        \n      \n    \n    \n    def forward1(self,x):\n        x=self.conv1(x)\n        x = self.batch_norm1(x)\n        x=F.relu(x)\n        x=self.pool1(x)\n        \n        x=self.conv2(x)\n        x = self.batch_norm2(x)\n        x=F.relu(x)\n        x=self.pool2(x)\n        \n        x=self.conv3(x)\n        x=F.relu(x)\n        \n        x = x.view(-1,80*2*2)\n        \n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        \n        x = F.log_softmax(x, dim=1)\n        \n        return x\n    \n    \n    def forward(self, input1, input2):\n        # forward pass of input 1\n        output1 = self.forward1(input1)\n        # forward pass of input 2\n        output2 = self.forward1(input2)\n        \n        return output1, output2\n","execution_count":37,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Constrastive Loss Function:-"},{"metadata":{},"cell_type":"markdown","source":"Contrastive loss:-\nContrastive loss is widely-used in unsupervised and self-supervised learning. Originally developed by Hadsell et al. in 2016 from Yann LeCun’s group, this loss function operates on pairs of samples instead of individual samples. It defines a binary indicator Y for each pair of samples stating whether they should be deemed similar, and a learnable distance function D_W(x_1, x_2) between a pair of samples x_1, x_2, parameterized by the weights W in the neural network. \n\n, where m>0 is a margin. The margin defines a radius around the embedding space of a sample so that dissimilar pairs of samples only contribute to the contrastive loss function if the distance D_W is within the margin.\n\n\nIntuitively, this loss function encourages the neural network to learn a embedding to place samples with the same labels close to each other, while distancing the samples with different labels in the embedding space."},{"metadata":{"trusted":true},"cell_type":"code","source":"class ContrastiveLoss(torch.nn.Module):\n    \n    def __init__(self, margin=2.0):\n        super(ContrastiveLoss, self).__init__()\n        self.margin = margin\n\n    def forward(self, output1, output2, label):\n        euclidean_distance = F.pairwise_distance(output1, output2)\n        loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +\n                                      (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))\n\n\n        return loss_contrastive","execution_count":38,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load the dataset as pytorch tensors using dataloader\ntrain_dataloader = DataLoader(dataset,\n                        shuffle=True,\n                        num_workers=8,\n                        batch_size=32)","execution_count":39,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataloader","execution_count":40,"outputs":[{"output_type":"execute_result","execution_count":40,"data":{"text/plain":"<torch.utils.data.dataloader.DataLoader at 0x7f8009d23a50>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check whether you have GPU is loaded or not\nif torch.cuda.is_available():\n    print('Yes')","execution_count":41,"outputs":[{"output_type":"stream","text":"Yes\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"net = SiameseNetwork().cuda()\n# Loss Function\ncriterion = ContrastiveLoss()                      # setting criterion\noptimizer = torch.optim.SGD(net.parameters(), lr = 3e-4) # setting optimizer\n# Declare Optimizer\noptimizer = optim.RMSprop(net.parameters(), lr=1e-4, alpha=0.99, eps=1e-8, weight_decay=0.0005, momentum=0.9)","execution_count":42,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train():\n    counter = []\n    loss_history = [] \n    iteration_number= 0\n    \n    for epoch in range(0,10):\n        for i, data in enumerate(train_dataloader,0):\n            img0, img1 , label = data\n            img0, img1 , label = img0.cuda(), img1.cuda() , label.cuda()\n            optimizer.zero_grad()\n            output1,output2 = net(img0,img1)\n            loss_contrastive = criterion(output1,output2,label)\n            loss_contrastive.backward()\n            optimizer.step()\n            if i %50 == 0 :\n                print(\"Epoch number {}\\n Current loss {}\\n\".format(epoch,loss_contrastive.item()))\n                iteration_number +=10\n                counter.append(iteration_number)\n                loss_history.append(loss_contrastive.item())\n    return net","execution_count":43,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = train()\ntorch.save(model.state_dict(), \"model.pt\")\nprint(\"Model Saved Successfully\")","execution_count":44,"outputs":[{"output_type":"stream","text":"Epoch number 0\n Current loss 1.7737867832183838\n\nEpoch number 0\n Current loss 1.2734043598175049\n\nEpoch number 0\n Current loss 1.2776978015899658\n\nEpoch number 0\n Current loss 1.5790718793869019\n\nEpoch number 0\n Current loss 1.4251023530960083\n\nEpoch number 0\n Current loss 1.3061398267745972\n\nEpoch number 0\n Current loss 1.5871816873550415\n\nEpoch number 0\n Current loss 1.439080834388733\n\nEpoch number 0\n Current loss 1.2262494564056396\n\nEpoch number 0\n Current loss 1.2726397514343262\n\nEpoch number 0\n Current loss 1.3761141300201416\n\nEpoch number 0\n Current loss 1.2373507022857666\n\nEpoch number 0\n Current loss 1.255690097808838\n\nEpoch number 0\n Current loss 1.1215015649795532\n\nEpoch number 0\n Current loss 1.1589336395263672\n\nEpoch number 1\n Current loss 1.4345885515213013\n\nEpoch number 1\n Current loss 1.1490474939346313\n\nEpoch number 1\n Current loss 1.3713948726654053\n\nEpoch number 1\n Current loss 1.3765206336975098\n\nEpoch number 1\n Current loss 1.3081318140029907\n\nEpoch number 1\n Current loss 1.3927184343338013\n\nEpoch number 1\n Current loss 1.1298203468322754\n\nEpoch number 1\n Current loss 1.1145000457763672\n\nEpoch number 1\n Current loss 1.4243377447128296\n\nEpoch number 1\n Current loss 1.42739737033844\n\nEpoch number 1\n Current loss 1.3799026012420654\n\nEpoch number 1\n Current loss 1.1462663412094116\n\nEpoch number 1\n Current loss 0.8842499852180481\n\nEpoch number 1\n Current loss 1.3265410661697388\n\nEpoch number 1\n Current loss 1.2667310237884521\n\nEpoch number 2\n Current loss 1.3624844551086426\n\nEpoch number 2\n Current loss 1.1025513410568237\n\nEpoch number 2\n Current loss 1.3150286674499512\n\nEpoch number 2\n Current loss 1.3508880138397217\n\nEpoch number 2\n Current loss 1.3219469785690308\n\nEpoch number 2\n Current loss 1.456710934638977\n\nEpoch number 2\n Current loss 1.3612991571426392\n\nEpoch number 2\n Current loss 1.3011544942855835\n\nEpoch number 2\n Current loss 1.331319808959961\n\nEpoch number 2\n Current loss 1.3290117979049683\n\nEpoch number 2\n Current loss 1.4475289583206177\n\nEpoch number 2\n Current loss 1.6069262027740479\n\nEpoch number 2\n Current loss 1.6882154941558838\n\nEpoch number 2\n Current loss 1.490556001663208\n\nEpoch number 2\n Current loss 1.5687832832336426\n\nEpoch number 3\n Current loss 1.0426112413406372\n\nEpoch number 3\n Current loss 1.358797550201416\n\nEpoch number 3\n Current loss 1.351595163345337\n\nEpoch number 3\n Current loss 1.317217230796814\n\nEpoch number 3\n Current loss 1.203218698501587\n\nEpoch number 3\n Current loss 1.2094776630401611\n\nEpoch number 3\n Current loss 1.2043737173080444\n\nEpoch number 3\n Current loss 1.3555761575698853\n\nEpoch number 3\n Current loss 1.3061490058898926\n\nEpoch number 3\n Current loss 1.3191120624542236\n\nEpoch number 3\n Current loss 1.1400121450424194\n\nEpoch number 3\n Current loss 1.4087554216384888\n\nEpoch number 3\n Current loss 1.5464690923690796\n\nEpoch number 3\n Current loss 1.404252290725708\n\nEpoch number 3\n Current loss 1.6789542436599731\n\nEpoch number 4\n Current loss 1.6729674339294434\n\nEpoch number 4\n Current loss 1.4947365522384644\n\nEpoch number 4\n Current loss 1.5172979831695557\n\nEpoch number 4\n Current loss 1.4427103996276855\n\nEpoch number 4\n Current loss 1.0841290950775146\n\nEpoch number 4\n Current loss 1.1554774045944214\n\nEpoch number 4\n Current loss 1.3312450647354126\n\nEpoch number 4\n Current loss 1.0467411279678345\n\nEpoch number 4\n Current loss 1.2234030961990356\n\nEpoch number 4\n Current loss 1.1373958587646484\n\nEpoch number 4\n Current loss 1.2971447706222534\n\nEpoch number 4\n Current loss 1.2746518850326538\n\nEpoch number 4\n Current loss 1.323789358139038\n\nEpoch number 4\n Current loss 1.2668492794036865\n\nEpoch number 4\n Current loss 1.3539538383483887\n\nEpoch number 5\n Current loss 1.1341402530670166\n\nEpoch number 5\n Current loss 1.3022618293762207\n\nEpoch number 5\n Current loss 1.3635250329971313\n\nEpoch number 5\n Current loss 1.2569290399551392\n\nEpoch number 5\n Current loss 1.1931565999984741\n\nEpoch number 5\n Current loss 1.3080379962921143\n\nEpoch number 5\n Current loss 1.5388946533203125\n\nEpoch number 5\n Current loss 1.593832015991211\n\nEpoch number 5\n Current loss 1.070658564567566\n\nEpoch number 5\n Current loss 1.2009977102279663\n\nEpoch number 5\n Current loss 1.4074676036834717\n\nEpoch number 5\n Current loss 1.1360538005828857\n\nEpoch number 5\n Current loss 1.1019537448883057\n\nEpoch number 5\n Current loss 1.1906994581222534\n\nEpoch number 5\n Current loss 1.2544902563095093\n\nEpoch number 6\n Current loss 1.2966351509094238\n\nEpoch number 6\n Current loss 1.2510950565338135\n\nEpoch number 6\n Current loss 1.4129531383514404\n\nEpoch number 6\n Current loss 1.1325949430465698\n\nEpoch number 6\n Current loss 1.4279717206954956\n\nEpoch number 6\n Current loss 1.5246833562850952\n\nEpoch number 6\n Current loss 1.294093132019043\n\nEpoch number 6\n Current loss 1.259851098060608\n\nEpoch number 6\n Current loss 1.430071473121643\n\nEpoch number 6\n Current loss 1.165215015411377\n\nEpoch number 6\n Current loss 1.293874979019165\n\nEpoch number 6\n Current loss 1.1692842245101929\n\nEpoch number 6\n Current loss 1.3607063293457031\n\nEpoch number 6\n Current loss 1.2551015615463257\n\nEpoch number 6\n Current loss 1.3593831062316895\n\nEpoch number 7\n Current loss 1.2661081552505493\n\nEpoch number 7\n Current loss 1.2073404788970947\n\nEpoch number 7\n Current loss 1.240318775177002\n\nEpoch number 7\n Current loss 1.3961135149002075\n\nEpoch number 7\n Current loss 1.0750045776367188\n\nEpoch number 7\n Current loss 1.200805425643921\n\nEpoch number 7\n Current loss 1.153618574142456\n\nEpoch number 7\n Current loss 1.2995076179504395\n\nEpoch number 7\n Current loss 1.6059746742248535\n\nEpoch number 7\n Current loss 1.1957886219024658\n\nEpoch number 7\n Current loss 1.1991384029388428\n\nEpoch number 7\n Current loss 1.195339560508728\n\nEpoch number 7\n Current loss 1.2493627071380615\n\nEpoch number 7\n Current loss 1.3851958513259888\n\nEpoch number 7\n Current loss 1.3417834043502808\n\nEpoch number 8\n Current loss 1.3025375604629517\n\nEpoch number 8\n Current loss 1.3531326055526733\n\nEpoch number 8\n Current loss 1.2518454790115356\n\nEpoch number 8\n Current loss 1.1289702653884888\n\nEpoch number 8\n Current loss 1.5092982053756714\n\nEpoch number 8\n Current loss 1.2034554481506348\n\nEpoch number 8\n Current loss 1.2518268823623657\n\nEpoch number 8\n Current loss 0.9229671955108643\n\nEpoch number 8\n Current loss 1.2543416023254395\n\nEpoch number 8\n Current loss 1.2499743700027466\n\nEpoch number 8\n Current loss 1.257411003112793\n\nEpoch number 8\n Current loss 1.2119109630584717\n\nEpoch number 8\n Current loss 1.4581036567687988\n\nEpoch number 8\n Current loss 1.144715428352356\n\nEpoch number 8\n Current loss 1.2996069192886353\n\nEpoch number 9\n Current loss 1.141677975654602\n\nEpoch number 9\n Current loss 1.2349509000778198\n\nEpoch number 9\n Current loss 1.2021375894546509\n\nEpoch number 9\n Current loss 1.3828219175338745\n\nEpoch number 9\n Current loss 1.0566279888153076\n\nEpoch number 9\n Current loss 1.192625880241394\n\nEpoch number 9\n Current loss 1.5025184154510498\n\nEpoch number 9\n Current loss 1.1983674764633179\n\nEpoch number 9\n Current loss 1.1338233947753906\n\nEpoch number 9\n Current loss 1.4084032773971558\n\nEpoch number 9\n Current loss 1.2986392974853516\n\nEpoch number 9\n Current loss 1.3965377807617188\n\nEpoch number 9\n Current loss 1.2591880559921265\n\nEpoch number 9\n Current loss 1.309883713722229\n\nEpoch number 9\n Current loss 1.3604012727737427\n\nModel Saved Successfully\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = SiameseNetwork().to(device)\nmodel.load_state_dict(torch.load(\"model.pt\"))","execution_count":45,"outputs":[{"output_type":"execute_result","execution_count":45,"data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"counter=0\nlist_0 = torch.FloatTensor([[0]])\nlist_1 = torch.FloatTensor([[1]])\nfor i, data in enumerate(test_dataloader,0): \n  x0, x1 , label = data\n  concatenated = torch.cat((x0,x1),0)\n  output1,output2 = model(x0.to(device),x1.to(device))\n  eucledian_distance = F.pairwise_distance(output1, output2)\n  if label==list_0:\n    label=\"Orginial\"\n  else:\n    label=\"Forged\"\n  imshow(torchvision.utils.make_grid(concatenated),'Dissimilarity: {:.2f} Label: {}'.format(eucledian_distance.item(),label))\n  counter=counter+1\n  if counter ==20:\n     break","execution_count":47,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'test_dataloader' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-47-cb88a72e029a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlist_0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlist_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx1\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mconcatenated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'test_dataloader' is not defined"]}]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}