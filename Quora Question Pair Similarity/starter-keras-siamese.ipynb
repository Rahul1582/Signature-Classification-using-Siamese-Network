{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/glove840b300dtxt/glove.840B.300d.txt\n",
      "/kaggle/input/quora-question-pairs/train.csv\n",
      "/kaggle/input/quora-question-pairs/sample_submission.csv\n",
      "/kaggle/input/quora-question-pairs/test.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding,Activation,Flatten,merge,TimeDistributed,CuDNNGRU,Bidirectional\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.layers import concatenate,subtract,add,maximum,multiply,Layer,Lambda\n",
    "from keras.backend import backend as K\n",
    "\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## read data from path\n",
    "MAX_SEQUENCE_LENGTH = 40\n",
    "MAX_NB_WORDS = 4000000\n",
    "nrows = 10000000\n",
    "EMBEDDING_DIM = 300\n",
    "\n",
    "# nrows = 10000\n",
    "\n",
    "path = '../input/quora-question-pairs/'\n",
    "train = pd.read_csv(path+\"train.csv\",nrows=nrows).astype(str)\n",
    "test = pd.read_csv(path+\"test.csv\",nrows=nrows).astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>Which fish would survive in salt water?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>Astrology: I am a Capricorn Sun Cap moon and c...</td>\n",
       "      <td>I'm a triple Capricorn (Sun, Moon and ascendan...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>Should I buy tiago?</td>\n",
       "      <td>What keeps childern active and far from phone ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "      <td>How can I be a good geologist?</td>\n",
       "      <td>What should I do to be a great geologist?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>17</td>\n",
       "      <td>18</td>\n",
       "      <td>When do you use シ instead of し?</td>\n",
       "      <td>When do you use \"&amp;\" instead of \"and\"?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>19</td>\n",
       "      <td>20</td>\n",
       "      <td>Motorola (company): Can I hack my Charter Moto...</td>\n",
       "      <td>How do I hack Motorola DCX3400 for free internet?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id qid1 qid2                                          question1  \\\n",
       "0  0    1    2  What is the step by step guide to invest in sh...   \n",
       "1  1    3    4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2  2    5    6  How can I increase the speed of my internet co...   \n",
       "3  3    7    8  Why am I mentally very lonely? How can I solve...   \n",
       "4  4    9   10  Which one dissolve in water quikly sugar, salt...   \n",
       "5  5   11   12  Astrology: I am a Capricorn Sun Cap moon and c...   \n",
       "6  6   13   14                                Should I buy tiago?   \n",
       "7  7   15   16                     How can I be a good geologist?   \n",
       "8  8   17   18                    When do you use シ instead of し?   \n",
       "9  9   19   20  Motorola (company): Can I hack my Charter Moto...   \n",
       "\n",
       "                                           question2 is_duplicate  \n",
       "0  What is the step by step guide to invest in sh...            0  \n",
       "1  What would happen if the Indian government sto...            0  \n",
       "2  How can Internet speed be increased by hacking...            0  \n",
       "3  Find the remainder when [math]23^{24}[/math] i...            0  \n",
       "4            Which fish would survive in salt water?            0  \n",
       "5  I'm a triple Capricorn (Sun, Moon and ascendan...            1  \n",
       "6  What keeps childern active and far from phone ...            0  \n",
       "7          What should I do to be a great geologist?            1  \n",
       "8              When do you use \"&\" instead of \"and\"?            0  \n",
       "9  How do I hack Motorola DCX3400 for free internet?            0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 95596 unique tokens\n"
     ]
    }
   ],
   "source": [
    "## generate keras inputs\n",
    "## tokenizer and padding\n",
    "corpus = []\n",
    "\n",
    "feats = ['question1','question2']\n",
    "for f in feats:\n",
    "    train[f] = train[f].astype(str)\n",
    "    test[f] = test[f].astype(str)\n",
    "    corpus+=train[f].values.tolist()\n",
    "    \n",
    "    \n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "X_q1 = tokenizer.texts_to_sequences(train['question1'])\n",
    "X_q2 = tokenizer.texts_to_sequences(train['question2'])\n",
    "\n",
    "X_test_q1 = tokenizer.texts_to_sequences(test['question1'])\n",
    "X_test_q2 = tokenizer.texts_to_sequences(test['question2'])\n",
    "\n",
    "\n",
    "X_q1 = pad_sequences(X_q1, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "X_q2 = pad_sequences(X_q2, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "X_test_q1 = pad_sequences(X_test_q1, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "X_test_q2 = pad_sequences(X_test_q2, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "y = train['is_duplicate'].values\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens' % len(word_index))\n",
    "nb_words = min(MAX_NB_WORDS, len(word_index))+1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## question \n",
    "    how to use pretrained embedding like glove,word2vec\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2196018it [05:37, 6512.26it/s]\n"
     ]
    }
   ],
   "source": [
    "def get_coefs(word, *arr):\n",
    "    return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "def load_embeddings(path):\n",
    "    with open(path) as f:\n",
    "        return dict(get_coefs(*line.strip().split(' ')) for line in tqdm(f))\n",
    "\n",
    "def build_matrix(word_index, path):\n",
    "    embedding_index = load_embeddings(path)\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "    unknown_words = []\n",
    "    \n",
    "    for word, i in word_index.items():\n",
    "        try:\n",
    "            embedding_matrix[i] = embedding_index[word]\n",
    "        except KeyError:\n",
    "            unknown_words.append(word)\n",
    "    return embedding_matrix, unknown_words\n",
    "\n",
    "glove_path = '../input/glove840b300dtxt/glove.840B.300d.txt'\n",
    "embedding_matrix,unknown_words = build_matrix(word_index,glove_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## eval metric is logloss\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(323432, 40) (80858, 40)\n"
     ]
    }
   ],
   "source": [
    "## validation set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train_q1,X_val_q1,X_train_q2,X_val_q2,y_train,y_val = train_test_split(X_q1,X_q2,y,train_size=0.8,random_state=1024)\n",
    "print(X_train_q1.shape,X_val_q1.shape)\n",
    "X_train = [X_train_q1,X_train_q2]\n",
    "X_val = [X_val_q1,X_val_q2]\n",
    "X_test = [X_test_q1,X_test_q2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_model_siamese(nb_words=50000,MAX_SEQUENCE_LENGTH=200,EMBEDDING_DIM=200,act='relu',embedding_matrix=None):\n",
    "    ########################################\n",
    "    ## define the model structure\n",
    "    ########################################\n",
    "    \n",
    "    input_q1 = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "    input_q2 = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "\n",
    "    if embedding_matrix is not None:\n",
    "        weights=[embedding_matrix],\n",
    "\n",
    "        embedding_layer = Embedding(nb_words,\n",
    "                EMBEDDING_DIM,\n",
    "                input_length=MAX_SEQUENCE_LENGTH,\n",
    "                weights=[embedding_matrix],\n",
    "                trainable=False)\n",
    "    else:\n",
    "        embedding_layer = Embedding(nb_words,\n",
    "                EMBEDDING_DIM,\n",
    "                input_length=MAX_SEQUENCE_LENGTH,\n",
    "                trainable=True)\n",
    "\n",
    "    embedded_sequences_q1 = embedding_layer(input_q1)\n",
    "    embedded_sequences_q2 = embedding_layer(input_q2)\n",
    "\n",
    "    \n",
    "    bilstm_layer = Bidirectional(CuDNNGRU(64,return_sequences=False))\n",
    "\n",
    "    x1 = bilstm_layer(embedded_sequences_q1)\n",
    "    x2 = bilstm_layer(embedded_sequences_q2)\n",
    "\n",
    "    diff = subtract([x1,x2])\n",
    "    summation = add([x1,x2])\n",
    "    \n",
    "    x = concatenate([x1,x2,diff,summation],1)\n",
    "\n",
    "    x = Dense(64,activation=act)(x)\n",
    "    \n",
    "    preds = Dense(1,activation='sigmoid')(x)\n",
    "\n",
    "\n",
    "    model = Model(inputs=[input_q1,input_q2],outputs=preds)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "            optimizer='adam',\n",
    "            metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 40)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 40)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 40, 300)      28679100    input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 128)          140544      embedding_1[0][0]                \n",
      "                                                                 embedding_1[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "subtract_1 (Subtract)           (None, 128)          0           bidirectional_1[0][0]            \n",
      "                                                                 bidirectional_1[1][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 128)          0           bidirectional_1[0][0]            \n",
      "                                                                 bidirectional_1[1][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 512)          0           bidirectional_1[0][0]            \n",
      "                                                                 bidirectional_1[1][0]            \n",
      "                                                                 subtract_1[0][0]                 \n",
      "                                                                 add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 64)           32832       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            65          dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 28,852,541\n",
      "Trainable params: 173,441\n",
      "Non-trainable params: 28,679,100\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Train on 323432 samples, validate on 80858 samples\n",
      "Epoch 1/250\n",
      "323432/323432 [==============================] - 40s 125us/step - loss: 0.4712 - accuracy: 0.7666 - val_loss: 0.4245 - val_accuracy: 0.7991\n",
      "Epoch 2/250\n",
      "323432/323432 [==============================] - 37s 113us/step - loss: 0.3899 - accuracy: 0.8174 - val_loss: 0.3920 - val_accuracy: 0.8161\n",
      "Epoch 3/250\n",
      "323432/323432 [==============================] - 37s 113us/step - loss: 0.3503 - accuracy: 0.8399 - val_loss: 0.3794 - val_accuracy: 0.8255\n",
      "Epoch 4/250\n",
      "323432/323432 [==============================] - 38s 117us/step - loss: 0.3199 - accuracy: 0.8559 - val_loss: 0.3737 - val_accuracy: 0.8311\n",
      "Epoch 5/250\n",
      "323432/323432 [==============================] - 37s 114us/step - loss: 0.2934 - accuracy: 0.8703 - val_loss: 0.3753 - val_accuracy: 0.8319\n",
      "Epoch 6/250\n",
      "323432/323432 [==============================] - 37s 113us/step - loss: 0.2706 - accuracy: 0.8818 - val_loss: 0.3805 - val_accuracy: 0.8355\n",
      "Epoch 7/250\n",
      "323432/323432 [==============================] - 37s 113us/step - loss: 0.2498 - accuracy: 0.8927 - val_loss: 0.3948 - val_accuracy: 0.8337\n",
      "Epoch 8/250\n",
      "323432/323432 [==============================] - 37s 113us/step - loss: 0.2310 - accuracy: 0.9016 - val_loss: 0.4143 - val_accuracy: 0.8353\n",
      "Epoch 9/250\n",
      "323432/323432 [==============================] - 37s 113us/step - loss: 0.2147 - accuracy: 0.9098 - val_loss: 0.4270 - val_accuracy: 0.8364\n"
     ]
    }
   ],
   "source": [
    "## train model\n",
    "\n",
    "early_stopping =EarlyStopping(monitor='val_loss', patience=5)\n",
    "bst_model_path = 'best_model.h5'\n",
    "model_checkpoint = ModelCheckpoint(bst_model_path, save_best_only=True, save_weights_only=True)\n",
    "\n",
    "model = get_model_siamese(nb_words=nb_words,MAX_SEQUENCE_LENGTH=MAX_SEQUENCE_LENGTH,EMBEDDING_DIM=EMBEDDING_DIM,act='relu',embedding_matrix=embedding_matrix)\n",
    "model.fit(X_train,y_train,epochs=250,validation_data=(X_val, y_val),batch_size=128,shuffle=True,verbose=1,callbacks=[early_stopping, model_checkpoint])\n",
    "\n",
    "model.load_weights(bst_model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## make submission \n",
    "y_pred_test = model.predict(X_test)\n",
    "\n",
    "submission = pd.DataFrame()\n",
    "submission['test_id'] = test['test_id']\n",
    "submission['is_duplicate'] = y_pred_test\n",
    "submission.to_csv('submission_siamese.csv',index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
